---
layout: post
title: "DeepSeek V3 paper 阅读记录"
date: 2025-01-29 10:00:09 +0800
categories: draft 
---

DeepSeek-V3 仍采用 **Multi-head Latent Attention（MLA）**进行有效推理，并采用 **DeepSeekMoE** 进行经济有效的训练（MLA 与 DeepSeekMoE 为 DeepSeek 在之前模型中提出的技术）。
除此之外，DeepSeek-V3 提出了 **auxiliary-loss-free** 策略用于负载均衡，目的是尽量减少因鼓励负载平衡而对模型性能造成的不利影响。 其次，DeepSeek-V3 采用了 **multi-token prediction training objective**，据我们观察，此机制提高了评估基准的整体性能。

FP8 混合精度训练

Pipeline bubble

在模型训练方面：

- 提出 DualPipe 算法减少通信时间
- 充分利用通信带宽
- 优化内存占用，避免昂贵的 Tensor 并行

Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computation communication overlap.

Conventional Transformer models usually adopts Multi-Head Attention (MHA), but during generation, its heavy Key-Value (KV) cache will become the bottleneck that limit the inference efficiency

Equipped with **low-rank key-value joint compression**, MLA achieves better performance than MHA, but requires a significantly smaller amount of KV cache.

### **MHA**

[如何理解attention中的Q,K,V？](https://www.zhihu.com/question/298810062/answer/86505956036)

目标：
- 减少串行计算（RNN）
- 学习较远位置间的依赖（CNN）

$d$ 为 embedding 维度，$d_h$ 为每个 attention head 的维度，$n_h$ 为 attention head 的数量，$h_t \in \mathbb{R}^d$ 为某个 attention layer 的第 $t$ 个 token 的 attention 输入，
${\bf q}_t,{\bf k}_t,{\bf v}_t \in \mathbb{R}^{d_h n_h}$，$W^Q,W^K,W^V \in \mathbb{R}^{d_h n_h \times d}$。

$$
{\bf q}_t = W^Q {\bf h}_t
$$

$$
{\bf k}_t = W^K {\bf h}_t
$$

$$
{\bf v}_t = W^V {\bf h}_t
$$

$$
W^*=
\begin{bmatrix}
W^*_1 \\
W^*_2 \\
\vdots \\
W^*_{n_h}
\end{bmatrix}, W^*_i \in \mathbb{R}^{d_h \times d}
$$

$$
{\bf q}_t=
\begin{bmatrix}
{\bf q}_{t,1} \\
{\bf q}_{t,2} \\
\vdots \\
{\bf q}_{t,n_h}
\end{bmatrix}, {\bf q}_{t,i}=W^Q_i {\bf h}_t
$$

$$
{\bf k}_t=
\begin{bmatrix}
{\bf k}_{t,1} \\
{\bf k}_{t,2} \\
\vdots \\
{\bf k}_{t,n_h}
\end{bmatrix}, {\bf k}_{t,i}=W^K_i {\bf h}_t
$$

$$
{\bf v}_t=
\begin{bmatrix}
{\bf v}_{t,1} \\
{\bf v}_{t,2} \\
\vdots \\
{\bf v}_{t,n_h}
\end{bmatrix}, {\bf v}_{t,i}=W^V_i {\bf h}_t
$$

{%
assign eq =
'$$
\begin{equation}
\begin{aligned}
{\bf o}_{t,i} &= \sum^t_{j=1} {\rm Softmax}_j (\frac{{\bf q}^T_{t,i} {\bf k}_{j,i}}{\sqrt{d_h}}) {\bf v}_{j,i} \\
&= \sum^t_{j=1}\left( \frac{{\rm exp}({\frac{{\bf q}^T_{t,i} {\bf k}_{j,i}}{\sqrt{d_h}}})}{\sum^t_{k=1}{\rm exp}({\frac{{\bf q}^T_{t,i} {\bf k}_{k,i}}{\sqrt{d_h}}})} \right) {\bf v}_{j,i} \\
&= \frac{ \sum^t_{j=1} {\rm exp}({\frac{{\bf q}^T_{t,i} {\bf k}_{j,i}}{\sqrt{d_h}}}) {\bf v}_{j,i}}{\sum^t_{k=1}{\rm exp}({\frac{{\bf q}^T_{t,i} {\bf k}_{k,i}}{\sqrt{d_h}}})} 
\end{aligned}
\end{equation}
$$'
%}
{{eq}}

$$
{\bf u}_t=W^O[{\bf o}_{t,1};{\bf o}_{t,2};\dots;{\bf o}_{t,n_h}]
$$

### **MLA**

[缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)

- 使用低秩矩阵分解降低 KV 缓存，以及推理过程中的内存占用
- 不直接对 Q、K 使用 RoPE，进行解耦

### **MoE**

MoE 是一种稀疏推理

### **Auxiliary-Loss-Free Load Balancing**

专家的负载不均衡会导致路由崩溃，且降低在**专家并行**场景中的计算效率。通常使用 Auxiliary Loss，但 Auxiliary Loss 如果太大会影响模型性能。

似乎如果过于保证负载均衡，会影响模型性能。为同时保证较好的负载均衡与模型性能，提出了 Auxiliary-Loss-Free Load Balancing。
