---
layout: post
title: "DeepSeek V3 paper 阅读记录"
date: 2025-01-29 10:00:09 +0800
categories: draft 
---

DeepSeek-V3 仍采用 **Multi-head Latent Attention（MLA）**进行有效推理，并采用 **DeepSeekMoE** 进行经济有效的训练。除此之外，DeepSeek-V3 提出了 **auxiliary-loss-free** 策略用于负载均衡，目的是尽量减少因鼓励负载平衡而对模型性能造成的不利影响。 其次，DeepSeek-V3 采用了 **multi-token prediction training objective**，据我们观察，此机制提高了评估基准的整体性能。

FP8 混合精度训练

Pipeline bubble

在模型训练方面：

- 提出 DualPipe 算法减少通信时间
- 充分利用通信带宽
- 优化内存占用，避免昂贵的 Tensor 并行

Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computation communication overlap.

Conventional Transformer models usually adopts Multi-Head Attention (MHA), but during generation, its heavy Key-Value (KV) cache will become the bottleneck that limit the inference efficiency

Equipped with **low-rank key-value joint compression**, MLA achieves better performance than MHA, but requires a significantly smaller amount of KV cache.

### **MHA**

目标：
- 减少串行计算（RNN）
- 学习较远位置间的依赖（CNN）

$d$ 为 embedding 维度，$d_h$ 为每个 attention head 的维度，$n_h$ 为 attention head 的数量，$h_t \in \mathbb{R}^d$ 为某个 attention layer 的第 $t$ 个 token 的 attention 输入，
${\bf q}_t,{\bf k}_t,{\bf v}_t \in \mathbb{R}^{d_h n_h}$，$W^Q,W^K,W^V \in \mathbb{R}^{d_h n_h \times d}$。

$$
{\bf q}_t = W^Q {\bf h}_t
$$

$$
{\bf k}_t = W^K {\bf h}_t
$$

$$
{\bf v}_t = W^V {\bf h}_t
$$

